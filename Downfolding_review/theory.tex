\newtheorem{theorem}{Theorem}
\newtheorem{definition}{Definition}


\section{Downfolding as a compression of the energy functional}
\label{sec:theory}
\subsection{Theory} 

Suppose we start with a quantum system with Hamiltonian $H$ and Hilbert space ${\mathcal H}$.

\begin{definition}
Let the energy functional be $E[\Psi] = \frac{\bra{\Psi}H\ket{\Psi}}{\braket{\Psi|\Psi}}$ for a wavefunction $\ket{\Psi} \in {\mathcal H}$.
\end{definition}


\begin{definition}
Let ${\mathcal LE}(H,N)$ be a subset of ${\mathcal H}$ spanned by $N$ vectors given by the lowest energy solutions to $H\ket{\Phi_i}=E_i{\Phi_i}$. 
\end{definition}

\begin{definition}
$H_{eff}$ is an operator on the Hilbert space ${\mathcal LE(H,N)}$.	 
\end{definition}


\begin{definition}
The effective model $E_{eff}[\Psi]=\frac{\bra{\Psi}H_{eff}\ket{\Psi}}{\braket{\Psi|\Psi}}$ is a functional from ${\mathcal LE} \rightarrow \mathbb{R}$
\end{definition}



\begin{theorem}
\label{theorem:criticalpoint}
$E[\Psi]$ has a critical point only where $\Psi$ is an eigenstate of $H$.
\end{theorem}
\begin{proof}
\begin{eqnarray}
\frac{\delta }{\delta \Psi^*}  E[\Psi] = \frac{\delta}{\delta \Psi^*}\frac{\langle \Psi |H|\Psi\rangle}{\langle \Psi |\Psi\rangle} = \frac{H|\Psi\rangle}{\langle \Psi |\Psi\rangle} - \langle \Psi |H|\Psi\rangle \frac{|\Psi \rangle}{|\langle \Psi | \Psi\rangle|^2} =\frac{ (H-E[\Psi])|\Psi\rangle }{\langle\Psi|\Psi\rangle}\,.
\end{eqnarray}
Therefore, 
$\frac{\delta }{\delta \Psi^*}  E[\Psi] = 0$ if and only if $(H-E[\Psi])|\Psi\rangle =0$, i.e., $\Psi$ is an eigenvector of $H$ corresponding to eigenvalue $E[\Psi]$.  
\end{proof}


%\begin{theorem} 	
%If $E_{eff}[\Psi] = E[\Psi], \forall \ket{\Psi} \in {\mathcal LE}$, then $H_{eff} \ket{\Phi_i} = E_i\ket{\Phi_i}$ for all eigenstates $\ket{\Phi_i}\in {\mathcal LE}$.
%\end{theorem}
%\begin{proof}
%	Suppose that $\ket{\Phi_i}$ is an eigenstate of $H$. Then 
%	\begin{equation}
%	\left.\frac{\delta E[\Psi]}{\delta\Psi}\right|_{\Psi=\Phi_i} = 0 
%	\end{equation}
%If this is true, then certainly the derivative on the low-energy manifold is also zero.
%\HJC{I am slightly confused. This is because the earlier derivative was in the space of all many body wavefunctions. But now the derivative is 
%constrained somehow because of loss of degrees of freedom.. }
%Since $E_{eff}[\Psi] = E[\Psi]$, 
%\begin{equation}
%	\left.\frac{\delta E_{eff}[\Psi]}{\delta\Psi}\right|_{\Psi=\Phi_i} = 0 
%\end{equation}
%for $\ket{\Psi} \in {\mathcal LE}$. Using Theorem~\ref{theorem:criticalpoint}, $H_{eff} \ket{\Phi_i} = E_i\ket{\Phi_i}$. 
%Similarly, if the derivative is not zero, then the wave function must not be an eigenstate.
%\end{proof}
\begin{theorem}
\label{theorem:matching}
Assume $\ket{\Psi } \in {\mathcal LE}$ and $E_{eff}[\Psi] = E[\Psi]$.
Then $H_{eff}\ket{\Psi}  = E \ket{\Psi} \iff H\ket{\Psi} = E\ket{\Psi}$.
\end{theorem}
\begin{proof}
Since $E_{eff}[\Psi] = E[\Psi]$, $\frac{\delta  E[\Psi] }{\delta \Psi^*}  = \frac{\delta E_{eff}[\Psi]}{\delta \Psi^*}  $.
If $\frac{\delta  E[\Psi] }{\delta \Psi^*} =0$, then $\frac{\delta E_{eff}[\Psi]}{\delta \Psi^*} =0$ since ${\mathcal LE}$ is a subset of ${\mathcal H}$.
If $\frac{\delta E_{eff}[\Psi]}{\delta \Psi^*} =0$, then write a state $\ket{\Phi}\in {\mathcal H} = \ket{\Psi} + \ket{\Psi_{HE}}$, where $\ket{\Psi}$ is the projection of $\ket{\Phi}$ into ${\mathcal LE}$. 
Then
\begin{align}
\left.\frac{\delta  E[\Phi] }{\delta \Phi^*}\right|_{\Phi=\Psi} =  \left[\frac{\delta  E[\Phi] }{\delta \Psi^*} + \frac{\delta  E[\Phi] }{\delta \Psi_{HE}^*} \right]_{\Phi=\Psi}.
\end{align}
\HHZ{This proof is not rigorous. if $y=a+b$, we don't have $\frac{\partial f}{\partial y} =\frac{\partial f}{\partial a} +\frac{\partial f}{\partial b} $. One needs to have rigorous chain rule to deduce some results like this. For example, define another variable $x=a-b$, ... }
$\frac{\delta  E[\Phi] }{\delta \Psi^*}=0$ because $\frac{\delta E_{eff}[\Psi]}{\delta \Psi^*} =0$. 
$\frac{\delta  E[\Phi] }{\delta \Psi_{HE}^*}=0$ can easily be verified by noting that $\ket{\Psi_{HE}}= \sum_{i=N+1}^\infty c_i \ket{\Phi_i}$.
Since the zeros coincide, the eigenstates also coincide. 
\end{proof}
\HHZ{The whole theorem does not work. You can see clearly if I write H as the following form, 
\begin{eqnarray}
H = 
\begin{pmatrix}
 H_{LE}  & H_c\\
H_c^\dagger & H_{HE}
\end{pmatrix}\,,
\end{eqnarray}
Then, based on all your definition, we have $H_{eff}=H_{LE}$. For sure this, satisfies the condition $E_{eff}[\Psi]= E[\Psi]$. But $H$ and $H_{eff}$ might not have coincide eigen spectrums if $H_c\ne0$. In other words, if and only if the high energy manifold and low energy manifold are decoupled with each other, we have coincide eigen spectrums for H and $H_{eff}$. 

In short, Theorem 2 does not work because when we apply $H$ to $\Psi$ (belong to LE), it in general will generate HE components. This is the same concern that Hitesh expressed in his comment. 
}

We have thus reduced the problem of finding an effective Hamiltonian $H_{eff}$ that reproduces the low-energy spectrum of $H$ to matching the corresponding energy functionals $E[\Psi]$ and $E_{eff}[\Psi]$. 
This involves both choosing the form of $H_{eff}$ and optimizing the parameters.
An important implication of this is that it is not necessary to diagonalize either of the Hamiltonians; one must only be able to select wave functions from the low-energy space ${\mathcal LE}$.
As we shall see, this can be substantially easier than attaining eigenstates.


The theory presented above maps coarse-graining into a functional approximation problem. 
This is still rather intimidating, since even supposing one can generate wave functions in the low-energy space, they are still complicated objects in a very large space.
An effective way to accomplish this is through the use of descriptors, $d_i[\Psi]$, which map from ${\mathcal H} \rightarrow \mathbb{R}$.
Then we can approximate the energy functional as follows
\begin{equation}
E_{eff}[\Psi] \simeq \sum_i f_i(d_i[\Psi]),
\end{equation}
where $f_i$ are some parameterized functions.
This will allow us to use techniques from statistical learning to efficiently describe $E_{eff}$. 
\subsection{Practical protocol}

\tikzstyle{decision} = [diamond, draw, fill=blue!10, 
    text width=4.5em, text badly centered, node distance=3cm, inner sep=0pt]
\tikzstyle{block} = [rectangle, draw, fill=blue!10, 
    text width=5em, text centered, rounded corners, minimum height=4em]
\tikzstyle{result} = [rectangle, draw, fill=red!10, 
    text width=5em, text centered, rounded corners, minimum height=4em]
\tikzstyle{line} = [draw,-latex',very thick]
\tikzstyle{cloud} = [draw, ellipse,fill=red!20, node distance=3cm,
    minimum height=2em]
\begin{figure*}
\begin{tikzpicture}[scale=2,node distance = 3cm, auto]
    % Place nodes
    \node [block] (wfs) {Generate $\ket{\Psi_i} \in {\mathcal LE}$};
    \node [block, right of=wfs] (descriptors) {Generate $d_j[\Psi_i]$,$E[\Psi_i]$};
    \node [block, right of=descriptors] (assess) {Assess descriptors};
    \node [block, right of=assess] (ansatz) {Ansatz: $E_i \simeq \sum_j p_j d_{ij}$};
    \node [block, right of=ansatz] (fit) {Fit optimal model};
    \node [result, right of=fit] (model) {Effective model};
    % Draw edges
    \path [line] (wfs) -- (descriptors);
    \path [line] (descriptors) -- (assess);
    \path [line] (assess) --  (ansatz);
    \path [line] (ansatz) --  (fit);
    \path [line] (fit) --  (model);

    \path [line] (assess.south) -- ($ (assess.south) + (0,-0.2)$) 
                 -- node [below] {Incomplete sampling} 
                 ($ (wfs.south) + (0,-0.2)$) --  (wfs.south);
    \path [line] (assess.north) -- ($ (assess.north) + (0,0.2)$) 
                 -- node [above] {Incomplete descriptor space} 
                 ($ (descriptors.north) + (0,0.2)$) --  (descriptors.north);

\end{tikzpicture}
\caption{A practical protocol for fitting effective models to {\it ab initio} data.}
\label{fig:protocol} 
\end{figure*}

A practical protocol is presented in Fig~\ref{fig:protocol}. 
In this section we go through this procedure step by step.

\paragraph{Generating $\ket{\Psi_i}\in {\mathcal LE}$}
Ideally one would be able to sample the entire low-energy space. 
Typically, however, the space will be too large and it will need to be sampled. 
The optimal wave functions to use depend on the models one expects to fit, which we will discuss in detail  in later steps. 
Simple strategies that we will use in the examples below include excitations with respect to a determinant and varying spin states.


\paragraph{Generate $d_j[\Psi_i]$ and $E[\Psi_i]$} 
The choice of descriptor is fundamental to the success of the downfolding. 
In the case of a second-quantized Hamiltonian
\begin{equation}
\hat{H}_{eff} = E_0 + \sum_{ij} t_{ij} (c_i^\dagger c_j + h.c.) + \sum_{ijkl} V_{ijkl} c_i^\dagger c_j^\dagger c_k c_l,
\end{equation}
a set of linear descriptors by simply taking the expectation value of both sides of the equation. 
Then for example, the occupation descriptor for orbital $k$ is $d_{occ(k)}[\Psi_i] = \braket{\Psi_i | c_k^\dagger c_k | \Psi}$; the double occupation descriptor for orbital $k$ is $d_{double(k)}[\Psi_i] = \braket{\Psi_i | n_{k\uparrow}n_{k\downarrow} | \Psi}$. 
The orbital that $c_k$ represents is part of the descriptor, and in the examples below we will discuss this choice as well.
One is not limited to static orbital descriptors; they may have a more complex functional dependence on the trial function to include orbital relaxation.

 
\paragraph{Assess descriptors}
At this point, one has collected the data $E_i$ and $d_{ij}$. 
If two descriptors have a large correlation coefficient, then they are redundant in the data set. 
This could either mean that the sampling of the low-energy Hilbert space ${\mathcal LE}$ was insufficient, or that they are both proxies for the same differences in states. 
If two data points have the same or very similar descriptor sets, but different energies, then either the descriptor set is not enough to describe the variations in the low-energy space, or the sampling has generated states that are not in the low-energy space.
To resolve these possibilities, one should analyze the difference between the two wave functions.  
Choosing the reduced set requires some intuition or somewhat advanced analysis of the many possibilities. 
In either case, when the model is accurate, the fits will be accurate.
If they are chosen incorrectly then intruder states can appear upon solution of the effective model. 
This can occur if descriptors values available in the reduced Hilbert space are not represented in the sampled wave functions. 
In that case, the model fitting is an extrapolation instead of an interpolation, which has the usual problems.
This is why it is desirable to have eigenstates in the set if possible; they are guaranteed to be on the corners of the descriptor space if the model is accurate.


\paragraph{Ansatz: $E_i \simeq \sum_i  d_{ij} p_j$} 
If the descriptors are chosen well, then the model can be written in linear form:
\begin{equation}
E[\Psi_i] = \sum_j p_j d_j[\Psi_i],	
\end{equation}
which we shorten to 
\begin{equation}
{\bf E} = D{\bf p} .
\label{eqn:EdP}
\end{equation}
If this can be done, the fitting problem is reduced to a linear regression optimization.
More complex functions of the descriptors are also possible, although at the cost of making the effective model more difficult to solve and complicating the fitting procedure.


\paragraph{Fit optimal model}
Finally, one wishes to find a set of parameters such that Eqn~\ref{eqn:EdP} is satisfied as closely as possible. 
There are many choices to make in this step, which will often depend on the desired properties of the final model. 
One can imagine choosing different cost functions to minimize, which can also include a penalty for complicated models. 
In our tests, we have successfully used LASSO \cite{Lasso} and matching pursuit techniques \cite{MP_Zhang1993} to select high quality and compact model parameters. 
A detailed example of using the latter technique is presented in Section~\ref{subsection:fese}.


